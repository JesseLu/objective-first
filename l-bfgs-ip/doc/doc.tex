\documentclass{article}
\usepackage{amsmath,amssymb}
\begin{document}
\title{Documentation of Objective-First Numerical Methods}
\author{Jesse Lu}
\maketitle
\tableofcontents

\section{Interior-point algorithm}
As taken from section 19.3 of \cite{NW04}, 
    the interior point method obtains step direction $p$ by solving
\begin{equation}
\begin{bmatrix}
    \nabla^2_{xx}\mathcal{L} & 0 & A_E^T(x) & A_I^T(x) \\
    0 & \Sigma & 0 & -I \\
    A_E(x) & 0 & 0 & 0 \\
    A_I(x) & -I & 0 & 0
\end{bmatrix}
\begin{bmatrix} p_x \\ p_s \\ -p_y \\ -p_z \end{bmatrix}
    = -
\begin{bmatrix}
    \nabla f(x) - A_E^T(x) y - A_I^T(x) z \\
    z - \mu S^{-1} e \\
    c_E(x) \\
    c_I(x) - s
\end{bmatrix}.
\end{equation}

This equation can be simplified by removing $p_s$ and then $p_z$.
The reduced system is then
\begin{multline}
\begin{bmatrix}
    \nabla^2_{xx}\mathcal{L} + A_I^T(x) \Sigma A_I^T(x) & A_E^T(x) \\
    A_E(x) & 0 
\end{bmatrix}
\begin{bmatrix} p_x \\ -p_y \end{bmatrix}
    = \\ -
\begin{bmatrix}
    \nabla f(x) - A_E^T(x) y - A_I(x) (z - \Sigma c_I(x) + \mu S^{-1} e) \\
    c_E(x)
\end{bmatrix},
\end{multline}
where
\begin{align}
    p_s &= A_I(x) p_x + c_I(x) - s \\
    p_z &= -\Sigma A_I(x) p_x - \Sigma c_I(x) + \mu S^{-1} e.
\end{align}

We can focus the problem by only considering 
    simple bound inequality constraints $l \le x \le u$, and
    affine equality constraints $A x - b = 0$.
Then our problem is written down as
\begin{equation}
\begin{bmatrix}
    \nabla^2 f(x) + \Sigma_0 + \Sigma_1 & A^T \\
    A & 0 
\end{bmatrix}
\begin{bmatrix} p_x \\ -p_y \end{bmatrix}
    = -
\begin{bmatrix}
    \nabla f(x) - A^T y + h_0 + h_1 \\ A x - b
\end{bmatrix},
\end{equation}
where
\begin{align}
h_0 &= -z_0 + \Sigma_0 (x - l) - \mu S_0^{-1} e \\
h_1 &= z_1 - \Sigma_1 (u - x) + \mu S_1^{-1} e, 
\end{align}
the other components of $p$ are
\begin{align}
    p_{s_0} &= p_x + (x - l) - s_0 \\
    p_{z_0} &= -\Sigma_0 p_x - \Sigma_0 (x - l) + \mu S_0^{-1} e \\
    p_{s_1} &= -p_x + (u - x) - s_1 \\
    p_{z_1} &= \Sigma_1 p_x - \Sigma_1 (u - x) + \mu S_1^{-1} e,
\end{align}
and the error function used is
\begin{multline}
E(x, s_0, s_1, y, z_0, z_1, \mu) = \text{max}\{ 
    \| \nabla f(x) - A^T y - z_0 + z_1 \|, \\
    \| S_0 z_0 - \mu e \|, 
    \| S_1 z_1 - \mu e \|, 
    \| A x - b \|, 
    \|(x - l) - s_0 \|, 
    \|(u - x) - s_1 \| \}.
\end{multline}

Lastly, inspired from section 11.7.3 of \cite{BL04}, 
    we perform a backtracking line search (see section 9.2 or \cite{BL04})
    in order to guarantee decrease of the residual
    $r(x^+, s_0^+, s_1^+, y^+, z_0^+, z_1^+, \mu)$ where,
    \begin{align*}
    x^+ &= x + t \alpha_p p_x \\
    s_0^+ &= s_0+t \alpha_p p_{s_0} \\
    s_1^+ &= s_1+t \alpha_p p_{s_1} \\
    y^+ &= y +\alpha_d p_y \\
    z_0^+ &= z_0 + \alpha_d p_{z_0} \\
    z_1^+ &= z_1+\alpha_d p_{z_1} 
    \end{align*}
    and,
\begin{multline}
    r(x, s_0, s_1, y, z_0, z_1, \mu) = \\
\left\|
\begin{bmatrix}
    \nabla f(x) - A^T y + (-z_0 + \Sigma_0 (x - l) - \mu S_0^{-1} e) 
                        + (z_1 - \Sigma_1 (u - x) + \mu S_1^{-1} e) \\
    A x - b
\end{bmatrix}\right\|_2.
\end{multline}
The exit condition for the line search is 
    \begin{equation} r(x^+, s_0^+, s_1^+, y^+, z_0^+, z_1^+, \mu) \le
    (1-\alpha t) r(x, s_0, s_1, y, z_0, z_1, \mu).
    \end{equation}
where $t$ is initially set to $t = \alpha_p$.

\section{Solving an augmented-arrow system}
We now address the solution of a system with the following form:
\begin{equation}
    \left(\begin{bmatrix} D & A^T \\ A & 0 \end{bmatrix} + UV^T\right)x = 
    (\hat{A} + UV^T)x = b. \label{eq:main}
\end{equation}

First, we obtain a method to solve $\hat{A}^{-1} x$.
We choose to use block substitution to do so. 
Such a method solves 
\begin{equation}
    \hat{A}y = 
    \begin{bmatrix} D & A^T \\ A & 0 \end{bmatrix}
    \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = 
    \begin{bmatrix} d_1 \\ d_2 \end{bmatrix}
\end{equation}
by computing
\begin{align}
    y_2 &= S^{-1} (d_2 - A D^{-1} d_1) \\
    y_1 &= D^{-1} ( d_1 - A^T y_2).
\end{align}

Next, we solve \ref{eq:main} by employing the matrix inversion lemma,
    \begin{equation}
    (A+UV^T)^{-1} = A^{-1} - A^{-1}U(I + V^T A^{-1}U)^{-1} V^T A^{-1},
    \end{equation}
    in the following way:
\begin{align}
    Y &= \hat{A}^{-1} U \\
    z &= \hat{A}^{-1} b \\
    x &= z - Y (I + V^T Y)^{-1} V^T z.
\end{align}

\section{Outer-product form of L-BFGS}
We use an outer-product form of the 
    limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm
    (See chapter 7.2 of \cite{NW04}).
This form produces a diagonal plus low-rank approximation
    of the Hessian of a function,
    \begin{equation}
    \tilde{H} = \delta I + W M W^T
    \end{equation}
    based on a limited sampling of previous gradients of the function.

% The values in the approximation, $\delta$, $W$, and $M$, are calculated from
%     a limited list of previous gradients, $\nabla f(x_i)$, as follows:

\section{Backtracking-line search}

\begin{thebibliography}{99}
\bibitem{NW04} Nocedal and Wright, 
    Numerical Optimization, Second Edition (Cambridge 2004)
\bibitem{BL04} Boyd and Vandenberghe,
    Convex Optimization (Cambridge 2004)
\end{thebibliography}
\end{document}
